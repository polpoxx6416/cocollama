{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"COCOLLAMA This library is a test library to make a personal CLI using ollama to use a local llm that can interact with your terminal. Check usage to check how it works.","title":"Home"},{"location":"#cocollama","text":"This library is a test library to make a personal CLI using ollama to use a local llm that can interact with your terminal. Check usage to check how it works.","title":"COCOLLAMA"},{"location":"api/","text":"API Reference CLI main () Command-line interface for Cocollama. Provides an interactive shell for communicating with Ollama LLM servers. Handles argument parsing and initializes the shell session. Arguments can be passed through command line --model: The Ollama model to use (defaults to \"gemma3:1b\") --host: Ollama server host (defaults to \"localhost\") --port: Ollama server port (defaults to \"11434\") Returns: \u2013 None Raises: KeyboardInterrupt \u2013 Handled gracefully for shell exit Source code in src/cocollama/cli.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def main (): \"\"\"Command-line interface for Cocollama. Provides an interactive shell for communicating with Ollama LLM servers. Handles argument parsing and initializes the shell session. Arguments can be passed through command line: --model: The Ollama model to use (defaults to \"gemma3:1b\") --host: Ollama server host (defaults to \"localhost\") --port: Ollama server port (defaults to \"11434\") Returns: None Raises: KeyboardInterrupt: Handled gracefully for shell exit \"\"\" parser = ArgumentParser ( description = \"Ollama CLI - Interactive shell for Ollama LLM server\" ) parser . add_argument ( \"--model\" , type = str , default = \"gemma3:1b\" , help = \"Ollama model to use (default: gemma3:1b)\" , ) parser . add_argument ( \"--host\" , type = str , default = \"localhost\" , help = \"Ollama host (default: localhost)\" ) parser . add_argument ( \"--port\" , type = str , default = \"11434\" , help = \"Ollama port (default: 11434)\" ) args = parser . parse_args () try : OllamaShell ( model = args . model , host = args . host , port = args . port ) . cmdloop () except KeyboardInterrupt : print ( \" \\n Exiting Ollama CLI\" ) sys . exit ( 0 ) Core Module OllamaShell Bases: Cmd Interactive shell for communicating with Ollama LLM server. Source code in src/cocollama/cocollama.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class OllamaShell ( cmd . Cmd ): \"\"\"Interactive shell for communicating with Ollama LLM server.\"\"\" def __init__ ( self , model : str = \"gemma3:1b\" , host : str = \"localhost\" , port : str = \"11434\" ): super () . __init__ () self . model = model self . host = host self . port = port self . base_url = f \"http:// { self . host } : { self . port } \" self . intro = f \"Ollama CLI - Connected to { self . base_url } - Using { model } \" self . prompt = \"cocollama> \" self . last_response = \"\" self . check_connection () def check_connection ( self ) -> None : \"\"\"Verify connection to Ollama server.\"\"\" try : response = requests . get ( url = urljoin ( self . base_url , \"/api/tags\" ), timeout = 5 ) response . raise_for_status () except ( requests . ConnectionError , requests . RequestException , requests . Timeout , ) as e : print ( f \"Error: { e } \\n Make sure ollama is running on { self . base_url } \" ) raise SystemExit ( 1 ) except Exception as e : print ( f \"Unexpected error: { e } \" ) raise SystemExit ( 1 ) def do_chat ( self , arg : str ) -> None : \"\"\"Send a message to the model: chat <your message>\"\"\" if not arg : print ( \"Please provide a message\" ) return try : processed_arg = self . process_commands ( arg ) data = { \"model\" : self . model , \"prompt\" : processed_arg , \"stream\" : True , } # print(f\"\\n[Prompt]: {processed_arg}\\n[Response]:\") self . _stream_response ( data ) except KeyboardInterrupt : print ( \" \\n Prompt interrupted\" ) except requests . RequestException as e : print ( f \"Request failed: { e } \" ) except Exception as e : print ( f \"Error: { e } \" ) def _stream_response ( self , data : Dict [ str , Any ]) -> None : \"\"\"Stream response from Ollama API.\"\"\" headers = { \"Content-Type\" : \"application/json\" } api_endpoint = urljoin ( self . base_url , \"/api/generate\" ) with requests . post ( url = api_endpoint , headers = headers , json = data , stream = True ) as response : response . raise_for_status () self . last_response = \"\" for line in response . iter_lines (): if not line : continue try : json_chunk = json . loads ( line . decode ( \"utf-8\" )) if not json_chunk . get ( \"done\" , True ): chunk_content = json_chunk . get ( \"response\" , \"\" ) print ( chunk_content , end = \"\" , flush = True ) self . last_response += chunk_content except json . JSONDecodeError : print ( f \" \\n Error decoding response: { line . decode ( 'utf-8' ) } \" ) print ( \" \\n \" ) def extract_commands ( self , text : str ) -> List [ str ]: \"\"\"Extract shell commands from input text using $() syntax.\"\"\" pattern = r \"\\$\\((.*?)\\)\" return re . findall ( pattern = pattern , string = text ) def process_commands ( self , text : str ) -> str : \"\"\"Process and execute embedded shell commands in the input text.\"\"\" commands = self . extract_commands ( text ) if not commands : return text print ( f \"Found commands to execute: { ', ' . join ( commands ) } \" ) response = input ( \"Do you want to run these commands? (y/n): \" ) . lower () . strip () if response != \"y\" : print ( \"Commands not executed.\" ) return text results = {} for cmd in commands : try : print ( f \"Executing: { cmd } \" ) result = subprocess . run ( cmd , shell = True , capture_output = True , text = True , timeout = 30 , # Prevent hanging on long-running commands ) cmd_id = cmd . replace ( \" \" , \"_\" ) if result . returncode == 0 : results [ cmd ] = ( f \"< { cmd_id } _output>_ { result . stdout . strip () } _</ { cmd_id } _output>\" ) else : results [ cmd ] = ( f \"< { cmd_id } _error>_ { result . stderr . strip () } _</ { cmd_id } _error>\" ) except subprocess . TimeoutExpired : results [ cmd ] = ( f \"< { cmd_id } _error>_Command timed out after 30 seconds_</ { cmd_id } _error>\" ) except Exception as e : results [ cmd ] = f \"< { cmd_id } _error>_Exception: { str ( e ) } _</ { cmd_id } _error>\" # Replace all command placeholders with their results processed_text = text for cmd in commands : processed_text = processed_text . replace ( f \"$( { cmd } )\" , results . get ( cmd , \"\" )) return processed_text def do_model ( self , arg : str ) -> None : \"\"\"Change the current model: model <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return try : # Verify the model exists with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () available_models = [ model [ \"name\" ] for model in result . get ( \"models\" , [])] if arg not in available_models : print ( f \"Model ' { arg } ' not found. Available models:\" ) for model in available_models : print ( f \" { model } \" ) return # Set the new model self . model = arg print ( f \"Model changed to: { self . model } \" ) except Exception as e : print ( f \"Error changing model: { e } \" ) def do_list ( self , arg : str ) -> None : \"\"\"List all available models\"\"\" try : with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () if not result . get ( \"models\" ): print ( \"No models found\" ) return print ( \"Available models:\" ) for model in result [ \"models\" ]: modified_date = ( datetime . fromisoformat ( model . get ( \"modified_at\" , \"unknown date\" ) . replace ( \"Z\" , \"+00:00\" ) ) . date () if model . get ( \"modified_at\" ) else \"unknown date\" ) print ( f \" { model [ 'name' ] : <15 } - { modified_date } \" ) except Exception as e : print ( f \"Error listing models: { e } \" ) def do_save ( self , arg : str ) -> None : \"\"\"Save last response to file: save <filename>\"\"\" if not arg : print ( \"Please provide a filename\" ) return if not self . last_response : print ( \"No response to save\" ) return try : with open ( arg , \"w\" , encoding = \"utf-8\" ) as f : f . write ( self . last_response ) print ( f \"Response saved to { arg } \" ) except Exception as e : print ( f \"Error saving response: { e } \" ) def do_clear ( self , arg : str ) -> None : \"\"\"Clear the screen\"\"\" os . system ( \"cls\" if os . name == \"nt\" else \"clear\" ) def do_exit ( self , arg : str ) -> bool : \"\"\"Exit the program\"\"\" print ( \"Goodbye!\" ) return True def do_pull ( self , arg : str ) -> None : \"\"\"Pull model from Ollama: pull <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return data = { \"model\" : arg , \"stream\" : True } url = urljoin ( self . base_url , \"/api/pull\" ) try : with requests . post ( url = url , json = data , stream = True ) as response : response . raise_for_status () for line in response . iter_lines (): if line : update = json . loads ( line ) status = update . get ( \"status\" ) if status : print ( status ) except requests . exceptions . HTTPError as e : print ( f \"HTTP Error: { e } \" ) except json . JSONDecodeError : print ( \"Error parsing response\" ) except Exception as e : print ( f \"Exception: { e } \" ) def do_remove ( self , arg : str ) -> None : f \"\"\"Remove model from { self . host } : remove <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return url = urljoin ( self . base_url , \"/api/delete\" ) data = { \"model\" : arg } try : with requests . delete ( url = url , json = data ) as response : response . raise_for_status () result = response . json () print ( result ) except requests . exceptions . HTTPError as e : print ( f \"HTTP Error: { e } \" ) except json . JSONDecodeError : print ( \"Error parsing response\" ) except Exception as e : print ( f \"Exception: { e } \" ) # Command aliases do_quit = do_exit do_ls = do_list check_connection () Verify connection to Ollama server. Source code in src/cocollama/cocollama.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def check_connection ( self ) -> None : \"\"\"Verify connection to Ollama server.\"\"\" try : response = requests . get ( url = urljoin ( self . base_url , \"/api/tags\" ), timeout = 5 ) response . raise_for_status () except ( requests . ConnectionError , requests . RequestException , requests . Timeout , ) as e : print ( f \"Error: { e } \\n Make sure ollama is running on { self . base_url } \" ) raise SystemExit ( 1 ) except Exception as e : print ( f \"Unexpected error: { e } \" ) raise SystemExit ( 1 ) do_chat ( arg ) Send a message to the model: chat Source code in src/cocollama/cocollama.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def do_chat ( self , arg : str ) -> None : \"\"\"Send a message to the model: chat <your message>\"\"\" if not arg : print ( \"Please provide a message\" ) return try : processed_arg = self . process_commands ( arg ) data = { \"model\" : self . model , \"prompt\" : processed_arg , \"stream\" : True , } # print(f\"\\n[Prompt]: {processed_arg}\\n[Response]:\") self . _stream_response ( data ) except KeyboardInterrupt : print ( \" \\n Prompt interrupted\" ) except requests . RequestException as e : print ( f \"Request failed: { e } \" ) except Exception as e : print ( f \"Error: { e } \" ) do_clear ( arg ) Clear the screen Source code in src/cocollama/cocollama.py 216 217 218 def do_clear ( self , arg : str ) -> None : \"\"\"Clear the screen\"\"\" os . system ( \"cls\" if os . name == \"nt\" else \"clear\" ) do_exit ( arg ) Exit the program Source code in src/cocollama/cocollama.py 220 221 222 223 def do_exit ( self , arg : str ) -> bool : \"\"\"Exit the program\"\"\" print ( \"Goodbye!\" ) return True do_list ( arg ) List all available models Source code in src/cocollama/cocollama.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def do_list ( self , arg : str ) -> None : \"\"\"List all available models\"\"\" try : with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () if not result . get ( \"models\" ): print ( \"No models found\" ) return print ( \"Available models:\" ) for model in result [ \"models\" ]: modified_date = ( datetime . fromisoformat ( model . get ( \"modified_at\" , \"unknown date\" ) . replace ( \"Z\" , \"+00:00\" ) ) . date () if model . get ( \"modified_at\" ) else \"unknown date\" ) print ( f \" { model [ 'name' ] : <15 } - { modified_date } \" ) except Exception as e : print ( f \"Error listing models: { e } \" ) do_model ( arg ) Change the current model: model Source code in src/cocollama/cocollama.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def do_model ( self , arg : str ) -> None : \"\"\"Change the current model: model <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return try : # Verify the model exists with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () available_models = [ model [ \"name\" ] for model in result . get ( \"models\" , [])] if arg not in available_models : print ( f \"Model ' { arg } ' not found. Available models:\" ) for model in available_models : print ( f \" { model } \" ) return # Set the new model self . model = arg print ( f \"Model changed to: { self . model } \" ) except Exception as e : print ( f \"Error changing model: { e } \" ) do_pull ( arg ) Pull model from Ollama: pull Source code in src/cocollama/cocollama.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def do_pull ( self , arg : str ) -> None : \"\"\"Pull model from Ollama: pull <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return data = { \"model\" : arg , \"stream\" : True } url = urljoin ( self . base_url , \"/api/pull\" ) try : with requests . post ( url = url , json = data , stream = True ) as response : response . raise_for_status () for line in response . iter_lines (): if line : update = json . loads ( line ) status = update . get ( \"status\" ) if status : print ( status ) except requests . exceptions . HTTPError as e : print ( f \"HTTP Error: { e } \" ) except json . JSONDecodeError : print ( \"Error parsing response\" ) except Exception as e : print ( f \"Exception: { e } \" ) do_save ( arg ) Save last response to file: save Source code in src/cocollama/cocollama.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def do_save ( self , arg : str ) -> None : \"\"\"Save last response to file: save <filename>\"\"\" if not arg : print ( \"Please provide a filename\" ) return if not self . last_response : print ( \"No response to save\" ) return try : with open ( arg , \"w\" , encoding = \"utf-8\" ) as f : f . write ( self . last_response ) print ( f \"Response saved to { arg } \" ) except Exception as e : print ( f \"Error saving response: { e } \" ) extract_commands ( text ) Extract shell commands from input text using $() syntax. Source code in src/cocollama/cocollama.py 96 97 98 99 def extract_commands ( self , text : str ) -> List [ str ]: \"\"\"Extract shell commands from input text using $() syntax.\"\"\" pattern = r \"\\$\\((.*?)\\)\" return re . findall ( pattern = pattern , string = text ) process_commands ( text ) Process and execute embedded shell commands in the input text. Source code in src/cocollama/cocollama.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def process_commands ( self , text : str ) -> str : \"\"\"Process and execute embedded shell commands in the input text.\"\"\" commands = self . extract_commands ( text ) if not commands : return text print ( f \"Found commands to execute: { ', ' . join ( commands ) } \" ) response = input ( \"Do you want to run these commands? (y/n): \" ) . lower () . strip () if response != \"y\" : print ( \"Commands not executed.\" ) return text results = {} for cmd in commands : try : print ( f \"Executing: { cmd } \" ) result = subprocess . run ( cmd , shell = True , capture_output = True , text = True , timeout = 30 , # Prevent hanging on long-running commands ) cmd_id = cmd . replace ( \" \" , \"_\" ) if result . returncode == 0 : results [ cmd ] = ( f \"< { cmd_id } _output>_ { result . stdout . strip () } _</ { cmd_id } _output>\" ) else : results [ cmd ] = ( f \"< { cmd_id } _error>_ { result . stderr . strip () } _</ { cmd_id } _error>\" ) except subprocess . TimeoutExpired : results [ cmd ] = ( f \"< { cmd_id } _error>_Command timed out after 30 seconds_</ { cmd_id } _error>\" ) except Exception as e : results [ cmd ] = f \"< { cmd_id } _error>_Exception: { str ( e ) } _</ { cmd_id } _error>\" # Replace all command placeholders with their results processed_text = text for cmd in commands : processed_text = processed_text . replace ( f \"$( { cmd } )\" , results . get ( cmd , \"\" )) return processed_text","title":"API Reference"},{"location":"api/#api-reference","text":"","title":"API Reference"},{"location":"api/#cli","text":"","title":"CLI"},{"location":"api/#cocollama.cli.main","text":"Command-line interface for Cocollama. Provides an interactive shell for communicating with Ollama LLM servers. Handles argument parsing and initializes the shell session. Arguments can be passed through command line --model: The Ollama model to use (defaults to \"gemma3:1b\") --host: Ollama server host (defaults to \"localhost\") --port: Ollama server port (defaults to \"11434\") Returns: \u2013 None Raises: KeyboardInterrupt \u2013 Handled gracefully for shell exit Source code in src/cocollama/cli.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def main (): \"\"\"Command-line interface for Cocollama. Provides an interactive shell for communicating with Ollama LLM servers. Handles argument parsing and initializes the shell session. Arguments can be passed through command line: --model: The Ollama model to use (defaults to \"gemma3:1b\") --host: Ollama server host (defaults to \"localhost\") --port: Ollama server port (defaults to \"11434\") Returns: None Raises: KeyboardInterrupt: Handled gracefully for shell exit \"\"\" parser = ArgumentParser ( description = \"Ollama CLI - Interactive shell for Ollama LLM server\" ) parser . add_argument ( \"--model\" , type = str , default = \"gemma3:1b\" , help = \"Ollama model to use (default: gemma3:1b)\" , ) parser . add_argument ( \"--host\" , type = str , default = \"localhost\" , help = \"Ollama host (default: localhost)\" ) parser . add_argument ( \"--port\" , type = str , default = \"11434\" , help = \"Ollama port (default: 11434)\" ) args = parser . parse_args () try : OllamaShell ( model = args . model , host = args . host , port = args . port ) . cmdloop () except KeyboardInterrupt : print ( \" \\n Exiting Ollama CLI\" ) sys . exit ( 0 )","title":"main"},{"location":"api/#core-module","text":"","title":"Core Module"},{"location":"api/#cocollama.cocollama.OllamaShell","text":"Bases: Cmd Interactive shell for communicating with Ollama LLM server. Source code in src/cocollama/cocollama.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 class OllamaShell ( cmd . Cmd ): \"\"\"Interactive shell for communicating with Ollama LLM server.\"\"\" def __init__ ( self , model : str = \"gemma3:1b\" , host : str = \"localhost\" , port : str = \"11434\" ): super () . __init__ () self . model = model self . host = host self . port = port self . base_url = f \"http:// { self . host } : { self . port } \" self . intro = f \"Ollama CLI - Connected to { self . base_url } - Using { model } \" self . prompt = \"cocollama> \" self . last_response = \"\" self . check_connection () def check_connection ( self ) -> None : \"\"\"Verify connection to Ollama server.\"\"\" try : response = requests . get ( url = urljoin ( self . base_url , \"/api/tags\" ), timeout = 5 ) response . raise_for_status () except ( requests . ConnectionError , requests . RequestException , requests . Timeout , ) as e : print ( f \"Error: { e } \\n Make sure ollama is running on { self . base_url } \" ) raise SystemExit ( 1 ) except Exception as e : print ( f \"Unexpected error: { e } \" ) raise SystemExit ( 1 ) def do_chat ( self , arg : str ) -> None : \"\"\"Send a message to the model: chat <your message>\"\"\" if not arg : print ( \"Please provide a message\" ) return try : processed_arg = self . process_commands ( arg ) data = { \"model\" : self . model , \"prompt\" : processed_arg , \"stream\" : True , } # print(f\"\\n[Prompt]: {processed_arg}\\n[Response]:\") self . _stream_response ( data ) except KeyboardInterrupt : print ( \" \\n Prompt interrupted\" ) except requests . RequestException as e : print ( f \"Request failed: { e } \" ) except Exception as e : print ( f \"Error: { e } \" ) def _stream_response ( self , data : Dict [ str , Any ]) -> None : \"\"\"Stream response from Ollama API.\"\"\" headers = { \"Content-Type\" : \"application/json\" } api_endpoint = urljoin ( self . base_url , \"/api/generate\" ) with requests . post ( url = api_endpoint , headers = headers , json = data , stream = True ) as response : response . raise_for_status () self . last_response = \"\" for line in response . iter_lines (): if not line : continue try : json_chunk = json . loads ( line . decode ( \"utf-8\" )) if not json_chunk . get ( \"done\" , True ): chunk_content = json_chunk . get ( \"response\" , \"\" ) print ( chunk_content , end = \"\" , flush = True ) self . last_response += chunk_content except json . JSONDecodeError : print ( f \" \\n Error decoding response: { line . decode ( 'utf-8' ) } \" ) print ( \" \\n \" ) def extract_commands ( self , text : str ) -> List [ str ]: \"\"\"Extract shell commands from input text using $() syntax.\"\"\" pattern = r \"\\$\\((.*?)\\)\" return re . findall ( pattern = pattern , string = text ) def process_commands ( self , text : str ) -> str : \"\"\"Process and execute embedded shell commands in the input text.\"\"\" commands = self . extract_commands ( text ) if not commands : return text print ( f \"Found commands to execute: { ', ' . join ( commands ) } \" ) response = input ( \"Do you want to run these commands? (y/n): \" ) . lower () . strip () if response != \"y\" : print ( \"Commands not executed.\" ) return text results = {} for cmd in commands : try : print ( f \"Executing: { cmd } \" ) result = subprocess . run ( cmd , shell = True , capture_output = True , text = True , timeout = 30 , # Prevent hanging on long-running commands ) cmd_id = cmd . replace ( \" \" , \"_\" ) if result . returncode == 0 : results [ cmd ] = ( f \"< { cmd_id } _output>_ { result . stdout . strip () } _</ { cmd_id } _output>\" ) else : results [ cmd ] = ( f \"< { cmd_id } _error>_ { result . stderr . strip () } _</ { cmd_id } _error>\" ) except subprocess . TimeoutExpired : results [ cmd ] = ( f \"< { cmd_id } _error>_Command timed out after 30 seconds_</ { cmd_id } _error>\" ) except Exception as e : results [ cmd ] = f \"< { cmd_id } _error>_Exception: { str ( e ) } _</ { cmd_id } _error>\" # Replace all command placeholders with their results processed_text = text for cmd in commands : processed_text = processed_text . replace ( f \"$( { cmd } )\" , results . get ( cmd , \"\" )) return processed_text def do_model ( self , arg : str ) -> None : \"\"\"Change the current model: model <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return try : # Verify the model exists with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () available_models = [ model [ \"name\" ] for model in result . get ( \"models\" , [])] if arg not in available_models : print ( f \"Model ' { arg } ' not found. Available models:\" ) for model in available_models : print ( f \" { model } \" ) return # Set the new model self . model = arg print ( f \"Model changed to: { self . model } \" ) except Exception as e : print ( f \"Error changing model: { e } \" ) def do_list ( self , arg : str ) -> None : \"\"\"List all available models\"\"\" try : with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () if not result . get ( \"models\" ): print ( \"No models found\" ) return print ( \"Available models:\" ) for model in result [ \"models\" ]: modified_date = ( datetime . fromisoformat ( model . get ( \"modified_at\" , \"unknown date\" ) . replace ( \"Z\" , \"+00:00\" ) ) . date () if model . get ( \"modified_at\" ) else \"unknown date\" ) print ( f \" { model [ 'name' ] : <15 } - { modified_date } \" ) except Exception as e : print ( f \"Error listing models: { e } \" ) def do_save ( self , arg : str ) -> None : \"\"\"Save last response to file: save <filename>\"\"\" if not arg : print ( \"Please provide a filename\" ) return if not self . last_response : print ( \"No response to save\" ) return try : with open ( arg , \"w\" , encoding = \"utf-8\" ) as f : f . write ( self . last_response ) print ( f \"Response saved to { arg } \" ) except Exception as e : print ( f \"Error saving response: { e } \" ) def do_clear ( self , arg : str ) -> None : \"\"\"Clear the screen\"\"\" os . system ( \"cls\" if os . name == \"nt\" else \"clear\" ) def do_exit ( self , arg : str ) -> bool : \"\"\"Exit the program\"\"\" print ( \"Goodbye!\" ) return True def do_pull ( self , arg : str ) -> None : \"\"\"Pull model from Ollama: pull <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return data = { \"model\" : arg , \"stream\" : True } url = urljoin ( self . base_url , \"/api/pull\" ) try : with requests . post ( url = url , json = data , stream = True ) as response : response . raise_for_status () for line in response . iter_lines (): if line : update = json . loads ( line ) status = update . get ( \"status\" ) if status : print ( status ) except requests . exceptions . HTTPError as e : print ( f \"HTTP Error: { e } \" ) except json . JSONDecodeError : print ( \"Error parsing response\" ) except Exception as e : print ( f \"Exception: { e } \" ) def do_remove ( self , arg : str ) -> None : f \"\"\"Remove model from { self . host } : remove <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return url = urljoin ( self . base_url , \"/api/delete\" ) data = { \"model\" : arg } try : with requests . delete ( url = url , json = data ) as response : response . raise_for_status () result = response . json () print ( result ) except requests . exceptions . HTTPError as e : print ( f \"HTTP Error: { e } \" ) except json . JSONDecodeError : print ( \"Error parsing response\" ) except Exception as e : print ( f \"Exception: { e } \" ) # Command aliases do_quit = do_exit do_ls = do_list","title":"OllamaShell"},{"location":"api/#cocollama.cocollama.OllamaShell.check_connection","text":"Verify connection to Ollama server. Source code in src/cocollama/cocollama.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def check_connection ( self ) -> None : \"\"\"Verify connection to Ollama server.\"\"\" try : response = requests . get ( url = urljoin ( self . base_url , \"/api/tags\" ), timeout = 5 ) response . raise_for_status () except ( requests . ConnectionError , requests . RequestException , requests . Timeout , ) as e : print ( f \"Error: { e } \\n Make sure ollama is running on { self . base_url } \" ) raise SystemExit ( 1 ) except Exception as e : print ( f \"Unexpected error: { e } \" ) raise SystemExit ( 1 )","title":"check_connection"},{"location":"api/#cocollama.cocollama.OllamaShell.do_chat","text":"Send a message to the model: chat Source code in src/cocollama/cocollama.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def do_chat ( self , arg : str ) -> None : \"\"\"Send a message to the model: chat <your message>\"\"\" if not arg : print ( \"Please provide a message\" ) return try : processed_arg = self . process_commands ( arg ) data = { \"model\" : self . model , \"prompt\" : processed_arg , \"stream\" : True , } # print(f\"\\n[Prompt]: {processed_arg}\\n[Response]:\") self . _stream_response ( data ) except KeyboardInterrupt : print ( \" \\n Prompt interrupted\" ) except requests . RequestException as e : print ( f \"Request failed: { e } \" ) except Exception as e : print ( f \"Error: { e } \" )","title":"do_chat"},{"location":"api/#cocollama.cocollama.OllamaShell.do_clear","text":"Clear the screen Source code in src/cocollama/cocollama.py 216 217 218 def do_clear ( self , arg : str ) -> None : \"\"\"Clear the screen\"\"\" os . system ( \"cls\" if os . name == \"nt\" else \"clear\" )","title":"do_clear"},{"location":"api/#cocollama.cocollama.OllamaShell.do_exit","text":"Exit the program Source code in src/cocollama/cocollama.py 220 221 222 223 def do_exit ( self , arg : str ) -> bool : \"\"\"Exit the program\"\"\" print ( \"Goodbye!\" ) return True","title":"do_exit"},{"location":"api/#cocollama.cocollama.OllamaShell.do_list","text":"List all available models Source code in src/cocollama/cocollama.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 def do_list ( self , arg : str ) -> None : \"\"\"List all available models\"\"\" try : with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () if not result . get ( \"models\" ): print ( \"No models found\" ) return print ( \"Available models:\" ) for model in result [ \"models\" ]: modified_date = ( datetime . fromisoformat ( model . get ( \"modified_at\" , \"unknown date\" ) . replace ( \"Z\" , \"+00:00\" ) ) . date () if model . get ( \"modified_at\" ) else \"unknown date\" ) print ( f \" { model [ 'name' ] : <15 } - { modified_date } \" ) except Exception as e : print ( f \"Error listing models: { e } \" )","title":"do_list"},{"location":"api/#cocollama.cocollama.OllamaShell.do_model","text":"Change the current model: model Source code in src/cocollama/cocollama.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 def do_model ( self , arg : str ) -> None : \"\"\"Change the current model: model <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return try : # Verify the model exists with requests . get ( url = urljoin ( self . base_url , \"/api/tags\" )) as response : response . raise_for_status () result = response . json () available_models = [ model [ \"name\" ] for model in result . get ( \"models\" , [])] if arg not in available_models : print ( f \"Model ' { arg } ' not found. Available models:\" ) for model in available_models : print ( f \" { model } \" ) return # Set the new model self . model = arg print ( f \"Model changed to: { self . model } \" ) except Exception as e : print ( f \"Error changing model: { e } \" )","title":"do_model"},{"location":"api/#cocollama.cocollama.OllamaShell.do_pull","text":"Pull model from Ollama: pull Source code in src/cocollama/cocollama.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def do_pull ( self , arg : str ) -> None : \"\"\"Pull model from Ollama: pull <model_name>\"\"\" if not arg : print ( f \"Current model: { self . model } \" ) return data = { \"model\" : arg , \"stream\" : True } url = urljoin ( self . base_url , \"/api/pull\" ) try : with requests . post ( url = url , json = data , stream = True ) as response : response . raise_for_status () for line in response . iter_lines (): if line : update = json . loads ( line ) status = update . get ( \"status\" ) if status : print ( status ) except requests . exceptions . HTTPError as e : print ( f \"HTTP Error: { e } \" ) except json . JSONDecodeError : print ( \"Error parsing response\" ) except Exception as e : print ( f \"Exception: { e } \" )","title":"do_pull"},{"location":"api/#cocollama.cocollama.OllamaShell.do_save","text":"Save last response to file: save Source code in src/cocollama/cocollama.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def do_save ( self , arg : str ) -> None : \"\"\"Save last response to file: save <filename>\"\"\" if not arg : print ( \"Please provide a filename\" ) return if not self . last_response : print ( \"No response to save\" ) return try : with open ( arg , \"w\" , encoding = \"utf-8\" ) as f : f . write ( self . last_response ) print ( f \"Response saved to { arg } \" ) except Exception as e : print ( f \"Error saving response: { e } \" )","title":"do_save"},{"location":"api/#cocollama.cocollama.OllamaShell.extract_commands","text":"Extract shell commands from input text using $() syntax. Source code in src/cocollama/cocollama.py 96 97 98 99 def extract_commands ( self , text : str ) -> List [ str ]: \"\"\"Extract shell commands from input text using $() syntax.\"\"\" pattern = r \"\\$\\((.*?)\\)\" return re . findall ( pattern = pattern , string = text )","title":"extract_commands"},{"location":"api/#cocollama.cocollama.OllamaShell.process_commands","text":"Process and execute embedded shell commands in the input text. Source code in src/cocollama/cocollama.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 def process_commands ( self , text : str ) -> str : \"\"\"Process and execute embedded shell commands in the input text.\"\"\" commands = self . extract_commands ( text ) if not commands : return text print ( f \"Found commands to execute: { ', ' . join ( commands ) } \" ) response = input ( \"Do you want to run these commands? (y/n): \" ) . lower () . strip () if response != \"y\" : print ( \"Commands not executed.\" ) return text results = {} for cmd in commands : try : print ( f \"Executing: { cmd } \" ) result = subprocess . run ( cmd , shell = True , capture_output = True , text = True , timeout = 30 , # Prevent hanging on long-running commands ) cmd_id = cmd . replace ( \" \" , \"_\" ) if result . returncode == 0 : results [ cmd ] = ( f \"< { cmd_id } _output>_ { result . stdout . strip () } _</ { cmd_id } _output>\" ) else : results [ cmd ] = ( f \"< { cmd_id } _error>_ { result . stderr . strip () } _</ { cmd_id } _error>\" ) except subprocess . TimeoutExpired : results [ cmd ] = ( f \"< { cmd_id } _error>_Command timed out after 30 seconds_</ { cmd_id } _error>\" ) except Exception as e : results [ cmd ] = f \"< { cmd_id } _error>_Exception: { str ( e ) } _</ { cmd_id } _error>\" # Replace all command placeholders with their results processed_text = text for cmd in commands : processed_text = processed_text . replace ( f \"$( { cmd } )\" , results . get ( cmd , \"\" )) return processed_text","title":"process_commands"},{"location":"usage/","text":"COCOLLAMA In your terminal just run cocollama when installed with pip in a venv or global env. It will be launched with your local ollama. to chat with your model just run chat <your_query> in the ollama cli. Run chat <query> $(cmd) to run a cmd and put the output as context inside your query. You can easily change the way your query is formatted in the extract_command method from OllamaShell . Default model is set to gemma3:1b . Be sure you have it loaded or use pull gemma3:1b . I you need help for a command type help <cmd> . It is better to run with uv with uv pip install . . I hope you enjoy this side project (made with the useful help of Claude ).","title":"Usage"},{"location":"usage/#cocollama","text":"In your terminal just run cocollama when installed with pip in a venv or global env. It will be launched with your local ollama. to chat with your model just run chat <your_query> in the ollama cli. Run chat <query> $(cmd) to run a cmd and put the output as context inside your query. You can easily change the way your query is formatted in the extract_command method from OllamaShell . Default model is set to gemma3:1b . Be sure you have it loaded or use pull gemma3:1b . I you need help for a command type help <cmd> . It is better to run with uv with uv pip install . . I hope you enjoy this side project (made with the useful help of Claude ).","title":"COCOLLAMA"}]}