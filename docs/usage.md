# COCOLLAMA

In your terminal just run `cocollama` when installed with pip in a venv or global env. 

It will be launched with your local ollama. 

to chat with your model just run `chat <your_query>` in the ollama cli. 